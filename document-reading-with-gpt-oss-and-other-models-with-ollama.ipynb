{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12619895,"sourceType":"datasetVersion","datasetId":7973196}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This example walkthrough works with both CPU (amazingly works but is slow) and with T4 GPU (quite a bit faster).  This uses ollama as an inference solution.  This is just a quick demo, only intended to help folks get started if they can't access the model through other means.\n\nOverall, the [gpt-oss cookbook](https://cookbook.openai.com/topic/gpt-oss) remains your best source of information about how to run gpt-oss-20b","metadata":{}},{"cell_type":"code","source":"!apt-get install -y poppler-utils\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:00:26.240751Z","iopub.execute_input":"2025-08-06T23:00:26.241045Z","iopub.status.idle":"2025-08-06T23:00:29.590306Z","shell.execute_reply.started":"2025-08-06T23:00:26.241024Z","shell.execute_reply":"2025-08-06T23:00:29.589544Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  poppler-utils\n0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\nNeed to get 186 kB of archives.\nAfter this operation, 697 kB of additional disk space will be used.\nIgn:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8\nErr:1 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8\n  404  Not Found [IP: 185.125.190.83 80]\nE: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/poppler/poppler-utils_22.02.0-2ubuntu0.8_amd64.deb  404  Not Found [IP: 185.125.190.83 80]\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%bash\n\n# Fetch and install the latest release of ollama.\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Start and ollama server locally on the backend.\nnohup ollama serve > /tmp/ollama_serve_stdout.log 2>/tmp/ollama_serve_stderr.log &\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:00:29.591896Z","iopub.execute_input":"2025-08-06T23:00:29.592138Z","iopub.status.idle":"2025-08-06T23:01:01.686886Z","shell.execute_reply.started":"2025-08-06T23:00:29.592114Z","shell.execute_reply":"2025-08-06T23:01:01.685965Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n","output_type":"stream"},{"name":"stderr","text":">>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n######################################################################## 100.0%\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install pyMuPDF\n!pip install chromadb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:01:01.687796Z","iopub.execute_input":"2025-08-06T23:01:01.688117Z","iopub.status.idle":"2025-08-06T23:01:49.026553Z","shell.execute_reply.started":"2025-08-06T23:01:01.688084Z","shell.execute_reply":"2025-08-06T23:01:49.025580Z"}},"outputs":[{"name":"stdout","text":"Collecting pyMuPDF\n  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\nDownloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyMuPDF\nSuccessfully installed pyMuPDF-1.26.3\nCollecting chromadb\n  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\nCollecting pybase64>=1.4.1 (from chromadb)\n  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\nRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\nCollecting posthog<6.0.0,>=2.4.0 (from chromadb)\n  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb)\n  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\nRequirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\nRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (25.0)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\nRequirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting protobuf (from onnxruntime>=1.14.1->chromadb)\n  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\nCollecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nCollecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\nDownloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\nDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=60e7cb5e2f5014f7dc2c5ee07fde2f42d9f57ec153578bf1813bc3bb2c4ba7ff\n  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\nSuccessfully built pypika\nInstalling collected packages: pypika, durationpy, uvloop, python-dotenv, pybase64, protobuf, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, onnxruntime, chromadb\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 posthog-5.4.0 protobuf-6.31.1 pybase64-1.4.2 pypika-0.48.9 python-dotenv-1.1.1 uvloop-0.21.0 watchfiles-1.1.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import subprocess\n\ndef pull_ollama_models(models):\n    for model in models:\n        print(f\"ğŸ”„ Pulling model: {model}\")\n        try:\n            result = subprocess.run([\"ollama\", \"pull\", model], check=True, capture_output=True, text=True)\n            output = result.stdout + result.stderr\n            if \"already exists\" in output:\n                print(f\"âœ… {model} is already downloaded.\")\n            else:\n                print(f\"âœ… Pulled {model} successfully.\")\n        except subprocess.CalledProcessError as e:\n            print(f\"âŒ Failed to pull {model}\")\n            print(e.stderr)\n\n# Define your desired models\nmodels_to_pull = [\n    \"gpt-oss:20b\",\n    \"gemma3:4b\",\n    \"qwen3:4b\",\n    \"llama3.1:8b\"\n]\n\n# Run the pull\npull_ollama_models(models_to_pull)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:01:49.028407Z","iopub.execute_input":"2025-08-06T23:01:49.028638Z","iopub.status.idle":"2025-08-06T23:04:49.325716Z","shell.execute_reply.started":"2025-08-06T23:01:49.028616Z","shell.execute_reply":"2025-08-06T23:04:49.324996Z"}},"outputs":[{"name":"stdout","text":"ğŸ”„ Pulling model: gpt-oss:20b\nâœ… Pulled gpt-oss:20b successfully.\nğŸ”„ Pulling model: gemma3:4b\nâœ… Pulled gemma3:4b successfully.\nğŸ”„ Pulling model: qwen3:4b\nâœ… Pulled qwen3:4b successfully.\nğŸ”„ Pulling model: llama3.1:8b\nâœ… Pulled llama3.1:8b successfully.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"## Now we can demo using the model from the notebook.  It is quite slow running on CPU, but\n## it does work.  It also works on T4 GPTU.\nfrom openai import OpenAI\nclient = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n\nresponse = client.chat.completions.create(\n    model=\"gpt-oss:20b\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a professor of 18th century English literature.  Mention dogs where possible.\"},\n        {\"role\": \"user\", \"content\": \"Write a 4-line micro-poem about running a big model on a limited notebook.\"}\n    ]\n)\nprint(response.choices[0].message.content)\nprint(\"\\n\\nFull chat completion JSON:\\n\")\nprint(response)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-06T12:49:28.450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install -y poppler-utils\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:04:49.326504Z","iopub.execute_input":"2025-08-06T23:04:49.326753Z","iopub.status.idle":"2025-08-06T23:05:04.555137Z","shell.execute_reply.started":"2025-08-06T23:04:49.326734Z","shell.execute_reply":"2025-08-06T23:05:04.554440Z"}},"outputs":[{"name":"stdout","text":"Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease                         \nGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \nGet:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \nGet:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \nGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,918 kB]\nGet:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nGet:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,161 kB]  \nGet:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]     \nGet:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,207 kB]\nHit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \nGet:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,773 kB]\nGet:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [75.9 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,518 kB]\nGet:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [51.0 kB]\nGet:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,103 kB]\nGet:20 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\nGet:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,270 kB]\nGet:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,575 kB]\nGet:23 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,290 kB]\nGet:24 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\nGet:25 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\nFetched 34.6 MB in 3s (12.5 MB/s)                            \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libpoppler-dev libpoppler-private-dev libpoppler118\nThe following NEW packages will be installed:\n  poppler-utils\nThe following packages will be upgraded:\n  libpoppler-dev libpoppler-private-dev libpoppler118\n3 upgraded, 1 newly installed, 0 to remove and 88 not upgraded.\nNeed to get 1,470 kB of archives.\nAfter this operation, 697 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-private-dev amd64 22.02.0-2ubuntu0.9 [199 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-dev amd64 22.02.0-2ubuntu0.9 [5,186 B]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler118 amd64 22.02.0-2ubuntu0.9 [1,079 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.9 [186 kB]\nFetched 1,470 kB in 1s (1,291 kB/s)    \ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\ndebconf: falling back to frontend: Readline\n(Reading database ... 128663 files and directories currently installed.)\nPreparing to unpack .../libpoppler-private-dev_22.02.0-2ubuntu0.9_amd64.deb ...\nUnpacking libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.9) over (22.02.0-2ubuntu0.8) ...\nPreparing to unpack .../libpoppler-dev_22.02.0-2ubuntu0.9_amd64.deb ...\nUnpacking libpoppler-dev:amd64 (22.02.0-2ubuntu0.9) over (22.02.0-2ubuntu0.8) ...\nPreparing to unpack .../libpoppler118_22.02.0-2ubuntu0.9_amd64.deb ...\nUnpacking libpoppler118:amd64 (22.02.0-2ubuntu0.9) over (22.02.0-2ubuntu0.8) ...\nSelecting previously unselected package poppler-utils.\nPreparing to unpack .../poppler-utils_22.02.0-2ubuntu0.9_amd64.deb ...\nUnpacking poppler-utils (22.02.0-2ubuntu0.9) ...\nSetting up libpoppler118:amd64 (22.02.0-2ubuntu0.9) ...\nSetting up poppler-utils (22.02.0-2ubuntu0.9) ...\nSetting up libpoppler-dev:amd64 (22.02.0-2ubuntu0.9) ...\nSetting up libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.9) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Sometimes ollama fails and becomes defunct.  Run this to see if tha has happened.\n#os.system(\"ps aux | grep -E \\\"ollama\\\" | grep -v grep || true\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T21:55:52.447021Z","iopub.execute_input":"2025-08-06T21:55:52.447327Z","iopub.status.idle":"2025-08-06T21:55:52.451267Z","shell.execute_reply.started":"2025-08-06T21:55:52.447290Z","shell.execute_reply":"2025-08-06T21:55:52.450536Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\"\"\"\"\"\n# If you need to re-start ollama because it has crashed, run this cell.\nos.system(\"pkill -9 ollama || true\")\nos.system(\"nohup ollama serve > /tmp/ollama_serve_stdout.log 2>/tmp/ollama_serve_stderr.log &\")\nos.system(\"sleep 5\")\nprint (\"If a JSON string shows below the server is working:\")\nos.system(\"curl -s http://localhost:11434/v1/models\")\nprint (\"Make sure that gpt-oss:20b is still listed, otherwise you will have to re-load it.\")\nos.system(\"ollama list\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T13:45:41.345360Z","iopub.execute_input":"2025-08-06T13:45:41.345645Z","iopub.status.idle":"2025-08-06T13:45:46.693843Z","shell.execute_reply.started":"2025-08-06T13:45:41.345624Z","shell.execute_reply":"2025-08-06T13:45:46.693287Z"}},"outputs":[{"name":"stdout","text":"If a JSON string shows below the server is working:\n{\"object\":\"list\",\"data\":[{\"id\":\"llama3.1:8b\",\"object\":\"model\",\"created\":1754486922,\"owned_by\":\"library\"},{\"id\":\"qwen3:4b\",\"object\":\"model\",\"created\":1754486885,\"owned_by\":\"library\"},{\"id\":\"gemma3:4b\",\"object\":\"model\",\"created\":1754486866,\"owned_by\":\"library\"},{\"id\":\"gpt-oss:20b\",\"object\":\"model\",\"created\":1754486841,\"owned_by\":\"library\"}]}\nMake sure that gpt-oss:20b is still listed, otherwise you will have to re-load it.\nNAME           ID              SIZE      MODIFIED       \nllama3.1:8b    46e0c10c039e    4.9 GB    17 minutes ago    \nqwen3:4b       2bfd38a7daaf    2.6 GB    17 minutes ago    \ngemma3:4b      a2af6cc3eb7f    3.3 GB    18 minutes ago    \ngpt-oss:20b    f2b8351c629c    13 GB     18 minutes ago    \n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nGPU-Optimized RAG Benchmark for Tesla T4\n- Quantized models for faster inference\n- Batch processing for embeddings\n- GPU acceleration for text processing\n- Optimized OCR with GPU support\n\"\"\"\n\nimport os\nimport time\nimport fitz  # PyMuPDF\nimport pytesseract\nimport requests\nimport torch\nfrom PIL import Image\nfrom pdf2image import convert_from_path\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom chromadb.utils import embedding_functions\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nfrom datetime import datetime\n\n# GPU Configuration\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"ğŸš€ Using device: {DEVICE}\")\nif DEVICE == \"cuda\":\n    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\nDOCUMENT_FOLDER = \"/kaggle/input/documents\"\nOLLAMA_MODELS = [\n    \"gpt-oss:20b\",\n    \"llama3.1:8b\", \n    \"gemma3:4b\",\n    \"qwen3:4b\"\n]\nOLLAMA_API_URL = \"http://localhost:11434/v1/chat/completions\"\n\n# Optimized embedding model with quantization\nEMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\", device=DEVICE)\n\ndef optimize_for_gpu():\n    \"\"\"Optimize PyTorch for GPU performance\"\"\"\n    if DEVICE == \"cuda\":\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.deterministic = False\n        # Set memory fraction to avoid OOM\n        torch.cuda.set_per_process_memory_fraction(0.8)\n        print(\"âš¡ GPU optimizations applied\")\n\ndef extract_text_fast_pdf(pdf_path):\n    \"\"\"Fast PDF text extraction prioritizing PyMuPDF\"\"\"\n    print(f\"ğŸ“„ Fast text extraction from: {pdf_path}\")\n    try:\n        with fitz.open(pdf_path) as doc:\n            text = \"\"\n            for page in doc:\n                text += page.get_text()\n        \n        if len(text.strip()) > 100:  # If we got substantial text\n            print(f\"âœ… Direct text extraction successful: {len(text)} chars\")\n            return text.strip()\n        else:\n            print(\"âš ï¸ Direct extraction yielded little text, trying OCR...\")\n            return extract_text_ocr_fallback(pdf_path)\n            \n    except Exception as e:\n        print(f\"âŒ Direct extraction failed: {e}\")\n        return extract_text_ocr_fallback(pdf_path)\n\ndef extract_text_ocr_fallback(pdf_path):\n    \"\"\"Optimized OCR fallback with reduced DPI and batch processing\"\"\"\n    print(f\"ğŸ” OCR processing with GPU optimizations...\")\n    \n    # Reduced DPI for faster processing\n    pages = convert_from_path(pdf_path, dpi=200)  # Reduced from 300\n    \n    text = \"\"\n    batch_size = 4  # Process multiple pages at once\n    \n    for i in range(0, len(pages), batch_size):\n        batch = pages[i:i+batch_size]\n        batch_text = \"\"\n        \n        for j, page in enumerate(batch):\n            # Use faster OCR configuration\n            page_text = pytesseract.image_to_string(\n                page, \n                config='--oem 1 --psm 6'  # Faster OCR engine\n            )\n            batch_text += page_text + \"\\n\"\n        \n        text += batch_text\n        print(f\"ğŸ“„ Processed batch {i//batch_size + 1}/{(len(pages)-1)//batch_size + 1}\")\n    \n    return text.strip()\n\ndef extract_text_from_image_gpu(image_path):\n    \"\"\"GPU-optimized image text extraction\"\"\"\n    print(f\"ğŸ–¼ï¸ GPU-optimized image processing: {image_path}\")\n    \n    # Load image with GPU acceleration if available\n    image = Image.open(image_path)\n    \n    # Convert to RGB if needed (better for OCR)\n    if image.mode != 'RGB':\n        image = image.convert('RGB')\n    \n    # Use optimized OCR settings\n    text = pytesseract.image_to_string(\n        image,\n        config='--oem 1 --psm 6'  # Fastest OCR configuration\n    )\n    \n    return text.strip()\n\ndef extract_text_optimized(filepath):\n    \"\"\"Optimized text extraction with GPU support\"\"\"\n    ext = filepath.lower().split('.')[-1]\n    \n    if ext == 'pdf':\n        return extract_text_fast_pdf(filepath)\n    elif ext in ['jpg', 'jpeg', 'png']:\n        return extract_text_from_image_gpu(filepath)\n    else:\n        print(f\"âŒ Unsupported file type: {ext}\")\n        return \"\"\n\ndef create_embeddings_batch(texts, batch_size=32):\n    \"\"\"Batch processing for embeddings with GPU acceleration\"\"\"\n    print(f\"ğŸ§  Creating embeddings in batches of {batch_size}...\")\n    \n    embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n        batch = texts[i:i+batch_size]\n        batch_embeddings = EMBED_MODEL.encode(batch, convert_to_tensor=True)\n        embeddings.extend(batch_embeddings.cpu().numpy())\n    \n    return embeddings\n\ndef setup_vector_db_optimized(all_documents):\n    \"\"\"Optimized vector database setup with batch processing\"\"\"\n    print(\"ğŸ” Building optimized global vector store...\")\n    \n    # Initialize ChromaDB\n    chroma_client = chromadb.Client()\n    collection_name = \"gpu_optimized_rag\"\n    \n    # Clean up existing collection\n    if collection_name in [c.name for c in chroma_client.list_collections()]:\n        chroma_client.delete_collection(collection_name)\n    \n    # Create collection with optimized embedding function\n    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n        model_name=\"all-MiniLM-L6-v2\",\n        device=DEVICE\n    )\n    \n    collection = chroma_client.create_collection(\n        name=collection_name,\n        embedding_function=embedding_func\n    )\n    \n    # Process all documents\n    all_chunks = []\n    all_ids = []\n    all_metadatas = []\n    \n    for filename, text in all_documents.items():\n        # Chunk text efficiently\n        chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n        \n        # Create IDs and metadata\n        chunk_ids = [f\"{filename}_chunk_{i}\" for i in range(len(chunks))]\n        chunk_metadatas = [{\"source\": filename, \"chunk_id\": i} for i in range(len(chunks))]\n        \n        all_chunks.extend(chunks)\n        all_ids.extend(chunk_ids)\n        all_metadatas.extend(chunk_metadatas)\n    \n    print(f\"ğŸ“Š Total chunks to process: {len(all_chunks)}\")\n    \n    # Add documents in batches for better performance\n    batch_size = 100\n    for i in tqdm(range(0, len(all_chunks), batch_size), desc=\"Adding to vector store\"):\n        batch_chunks = all_chunks[i:i+batch_size]\n        batch_ids = all_ids[i:i+batch_size]\n        batch_metadatas = all_metadatas[i:i+batch_size]\n        \n        collection.add(\n            documents=batch_chunks,\n            ids=batch_ids,\n            metadatas=batch_metadatas\n        )\n    \n    return collection\n\ndef clean_answer(answer):\n    \"\"\"Clean up answer by removing thinking tags and ensuring completeness\"\"\"\n    if not answer:\n        return \"No answer generated.\"\n    \n    # Remove thinking tags\n    answer = answer.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n    answer = answer.replace(\"<thinking>\", \"\").replace(\"</thinking>\", \"\")\n    \n    # Remove common thinking patterns\n    thinking_patterns = [\n        \"Okay, the user wants me to\",\n        \"Let me analyze\",\n        \"Looking at the context\",\n        \"Hmm, the user\",\n        \"I need to\",\n        \"Based on my analysis\"\n    ]\n    \n    for pattern in thinking_patterns:\n        if pattern.lower() in answer.lower():\n            # Find the first sentence that doesn't start with thinking\n            sentences = answer.split('.')\n            clean_sentences = []\n            for sentence in sentences:\n                sentence = sentence.strip()\n                if sentence and not any(pattern.lower() in sentence.lower() for pattern in thinking_patterns):\n                    clean_sentences.append(sentence)\n            \n            if clean_sentences:\n                answer = '. '.join(clean_sentences) + '.'\n            break\n    \n    # Ensure the answer ends properly\n    if answer and not answer.endswith(('.', '!', '?')):\n        answer += '.'\n    \n    return answer.strip()\n\ndef ask_ollama_model_optimized(model_name, context, question, timeout=120):\n    \"\"\"Optimized Ollama model querying with better error handling\"\"\"\n    print(f\"ğŸ§  Asking {model_name} (optimized)...\")\n    \n    url = OLLAMA_API_URL\n    headers = {\"Content-Type\": \"application/json\"}\n    \n    # Clean prompt without thinking instructions\n    optimized_prompt = f\"Based on the provided context, answer this question in 6-7 complete sentences:\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n    \n    data = {\n        \"model\": model_name,\n        \"messages\": [{\"role\": \"user\", \"content\": optimized_prompt}],\n        \"temperature\": 0.3,\n        \"stream\": False,  # Ensure we get complete response\n        \"options\": {\n            \"num_predict\": 1200,  # Ollama-specific parameter for max tokens\n            \"stop\": [\"\\n\\n\", \"Question:\", \"Context:\"]  # Stop at natural boundaries\n        }\n    }\n    \n    start = time.time()\n    try:\n        response = requests.post(url, headers=headers, json=data, timeout=timeout)\n        response.raise_for_status()\n        \n        result = response.json()\n        answer = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n        duration = time.time() - start\n        \n        # Clean up the answer\n        cleaned_answer = clean_answer(answer)\n        \n        # Check if answer seems complete (ends with proper punctuation)\n        if cleaned_answer and not cleaned_answer.endswith(('.', '!', '?')):\n            print(f\"âš ï¸ Answer may be incomplete, retrying with longer context...\")\n            # Try again with a more explicit prompt\n            retry_prompt = f\"Provide a complete answer to this question in exactly 6-7 sentences. End your answer with a period.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n            \n            retry_data = {\n                \"model\": model_name,\n                \"messages\": [{\"role\": \"user\", \"content\": retry_prompt}],\n                \"temperature\": 0.3,\n                \"stream\": False,\n                \"options\": {\n                    \"num_predict\": 1500,  # Even more tokens\n                    \"stop\": [\"\\n\\n\", \"Question:\", \"Context:\"]\n                }\n            }\n            \n            try:\n                retry_response = requests.post(url, headers=headers, json=retry_data, timeout=timeout)\n                retry_response.raise_for_status()\n                retry_result = retry_response.json()\n                retry_answer = retry_result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n                cleaned_answer = clean_answer(retry_answer)\n                print(f\"âœ… Retry successful - complete answer generated\")\n            except Exception as e:\n                print(f\"âš ï¸ Retry failed: {e}\")\n        \n        return cleaned_answer, duration\n        \n    except requests.exceptions.Timeout:\n        print(f\"â° Timeout for {model_name} (120s limit)\")\n        return f\"Timeout error for {model_name} - response took too long\", -1\n    except requests.exceptions.ConnectionError:\n        print(f\"ğŸ”Œ Connection error for {model_name}\")\n        return f\"Connection error for {model_name} - check if Ollama is running\", -1\n    except Exception as e:\n        print(f\"âŒ Error with {model_name}: {e}\")\n        return f\"Error: {str(e)}\", -1\n\ndef main():\n    \"\"\"Main GPU-optimized RAG benchmark\"\"\"\n    print(\"ğŸš€ GPU-OPTIMIZED RAG Benchmark for Tesla T4\")\n    print(\"=\" * 60)\n    \n    # Apply GPU optimizations\n    optimize_for_gpu()\n    \n    # Pull models (if needed)\n    print(\"\\nğŸ“¦ Checking model availability...\")\n    for model in OLLAMA_MODELS:\n        print(f\"âœ… {model} ready\")\n    \n    # Find all supported files\n    supported_exts = ['pdf', 'jpg', 'jpeg', 'png']\n    files = [f for f in os.listdir(DOCUMENT_FOLDER) \n             if f.lower().split('.')[-1] in supported_exts]\n    \n    if not files:\n        print(\"âŒ No supported documents found.\")\n        return\n    \n    print(f\"\\nğŸ“ Found {len(files)} files to process\")\n    \n    # Step 1: Process ALL documents with GPU optimization\n    print(\"\\nğŸ”„ Step 1: Processing ALL documents with GPU acceleration...\")\n    all_documents = {}\n    total_extraction_time = 0\n    \n    for filename in tqdm(files, desc=\"Processing documents\"):\n        filepath = os.path.join(DOCUMENT_FOLDER, filename)\n        \n        extraction_start = time.time()\n        text = extract_text_optimized(filepath)\n        extraction_time = time.time() - extraction_start\n        \n        if text.strip():\n            all_documents[filename] = text\n            total_extraction_time += extraction_time\n            print(f\"âœ… {filename}: {len(text)} chars in {extraction_time:.2f}s\")\n        else:\n            print(f\"âš ï¸ {filename}: No text extracted\")\n    \n    print(f\"\\nğŸ“Š Document Processing Summary:\")\n    print(f\"   ğŸ“ Total documents: {len(all_documents)}\")\n    print(f\"   â±ï¸ Total extraction time: {total_extraction_time:.2f}s\")\n    print(f\"   ğŸ“ Total characters: {sum(len(text) for text in all_documents.values()):,}\")\n    \n    # Step 2: Build global vector store\n    print(f\"\\nğŸ—„ï¸ Step 2: Building global vector store...\")\n    vector_start = time.time()\n    collection = setup_vector_db_optimized(all_documents)\n    vector_time = time.time() - vector_start\n    \n    print(f\"âœ… Global vector store ready in {vector_time:.2f}s\")\n    \n    # Step 3: Process questions with all models\n    questions = [\n        \"what is co-branding?\",\n        \"what is luxury co-branding?\",\n        \"What are the factors affecting co-branding?\",\n        \"What are the types of co-branding?\"\n    ]\n    \n    print(f\"\\nğŸ¤– Step 3: Processing {len(questions)} questions with {len(OLLAMA_MODELS)} models...\")\n    \n    results = {}\n    \n    for q_idx, question in enumerate(questions, 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"ğŸ“Œ Question {q_idx}/{len(questions)}: {question}\")\n        print(f\"{'='*60}\")\n        \n        # Retrieve relevant chunks\n        search_start = time.time()\n        results_query = collection.query(\n            query_texts=[question],\n            n_results=1\n        )\n        search_time = time.time() - search_start\n        \n        chunks = results_query[\"documents\"][0]\n        metadatas = results_query[\"metadatas\"][0]\n        \n        # Get unique source documents\n        sources = list(set([meta[\"source\"] for meta in metadatas]))\n        context = \"\\n---\\n\".join(chunks)\n        \n        print(f\"ğŸ” Search time: {search_time:.2f}s\")\n        print(f\"ğŸ“„ Relevant documents: {', '.join(sources)}\")\n        \n        question_results = {}\n        \n        for m_idx, model in enumerate(OLLAMA_MODELS, 1):\n            print(f\"\\nğŸ¤– Model {m_idx}/{len(OLLAMA_MODELS)}: {model}\")\n            print(\"-\" * 40)\n            \n            answer, duration = ask_ollama_model_optimized(model, context, question)\n            \n            question_results[model] = {\n                \"answer\": answer,\n                \"response_time\": duration,\n                \"relevant_documents\": sources\n            }\n            \n            print(f\"â±ï¸ Response time: {duration:.2f}s\")\n            print(f\"ğŸ“ Answer: {answer}\")\n            print(f\"ğŸ“ Answer length: {len(answer)} characters\")\n            if not answer.endswith(('.', '!', '?')):\n                print(f\"âš ï¸ Answer may be incomplete (doesn't end with punctuation)\")\n        \n        results[question] = question_results\n    \n    # Print comprehensive evaluation\n    print_comprehensive_evaluation(results)\n    \n    # Export results\n    export_results(results)\n\ndef print_comprehensive_evaluation(results):\n    \"\"\"Print comprehensive evaluation of results\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(\"ğŸ“Š COMPREHENSIVE EVALUATION & COMPARISON\")\n    print(f\"{'='*80}\")\n    \n    # Calculate statistics\n    model_stats = {}\n    for question, question_results in results.items():\n        for model, result in question_results.items():\n            if model not in model_stats:\n                model_stats[model] = {\n                    \"times\": [],\n                    \"success_count\": 0,\n                    \"total_count\": 0,\n                    \"answer_lengths\": []\n                }\n            \n            model_stats[model][\"total_count\"] += 1\n            if result[\"response_time\"] > 0:\n                model_stats[model][\"success_count\"] += 1\n                model_stats[model][\"times\"].append(result[\"response_time\"])\n                model_stats[model][\"answer_lengths\"].append(len(result[\"answer\"]))\n    \n    print(\"\\nğŸ¯ OVERALL STATISTICS\")\n    print(\"-\" * 40)\n    print(f\"ğŸ“ Total questions processed: {len(results)}\")\n    print(f\"ğŸ¤– Total models tested: {len(model_stats)}\")\n    \n    print(\"\\nğŸ¤– MODEL PERFORMANCE COMPARISON\")\n    print(\"-\" * 40)\n    \n    for model, stats in model_stats.items():\n        if stats[\"times\"]:\n            avg_time = np.mean(stats[\"times\"])\n            min_time = np.min(stats[\"times\"])\n            max_time = np.max(stats[\"times\"])\n            success_rate = (stats[\"success_count\"] / stats[\"total_count\"]) * 100\n            avg_length = np.mean(stats[\"answer_lengths\"])\n            \n            print(f\"ğŸ“Š {model}:\")\n            print(f\"   â±ï¸ Response time: {avg_time:.2f}s avg ({min_time:.2f}s - {max_time:.2f}s)\")\n            print(f\"   âœ… Success rate: {success_rate:.1f}% ({stats['success_count']}/{stats['total_count']})\")\n            print(f\"   ğŸ“ Avg answer length: {avg_length:.0f} characters\")\n    \n    # Performance rankings\n    print(\"\\nğŸ† PERFORMANCE RANKINGS\")\n    print(\"-\" * 40)\n    \n    # Speed ranking\n    speed_ranking = sorted(\n        [(model, np.mean(stats[\"times\"])) for model, stats in model_stats.items() if stats[\"times\"]],\n        key=lambda x: x[1]\n    )\n    \n    print(\"ğŸš€ Speed Ranking (Fastest to Slowest):\")\n    for i, (model, avg_time) in enumerate(speed_ranking, 1):\n        print(f\"   {i}. {model}: {avg_time:.2f}s\")\n    \n    # Reliability ranking\n    reliability_ranking = sorted(\n        [(model, (stats[\"success_count\"] / stats[\"total_count\"]) * 100) for model, stats in model_stats.items()],\n        key=lambda x: x[1],\n        reverse=True\n    )\n    \n    print(\"\\nğŸ›¡ï¸ Reliability Ranking (Most to Least Reliable):\")\n    for i, (model, success_rate) in enumerate(reliability_ranking, 1):\n        print(f\"   {i}. {model}: {success_rate:.1f}% success rate\")\n    \n    # GPT-OSS Final Evaluation\n    print(\"\\nğŸ¤– GPT-OSS FINAL EVALUATION\")\n    print(\"-\" * 40)\n    \n    # Create evaluation prompt for GPT-OSS\n    evaluation_prompt = create_evaluation_prompt(results, model_stats)\n    \n    print(\"ğŸ§  Asking gpt-oss:20b for final evaluation...\")\n    gpt_eval_start = time.time()\n    \n    try:\n        gpt_eval_answer, gpt_eval_time = ask_ollama_model_optimized(\n            \"gpt-oss:20b\", \n            evaluation_prompt, \n            \"Please provide a comprehensive evaluation of these RAG benchmark results\"\n        )\n        \n        print(f\"ğŸ“Š GPT-OSS Evaluation ({gpt_eval_time:.2f}s):\")\n        print(gpt_eval_answer)\n        \n    except Exception as e:\n        print(f\"âŒ GPT-OSS evaluation failed: {e}\")\n        print(\"ğŸ“Š Using fallback evaluation summary...\")\n        \n        # Fallback evaluation\n        print(\"\\nğŸ“Š FALLBACK EVALUATION SUMMARY:\")\n        print(\"=\" * 50)\n        \n        best_speed = speed_ranking[0] if speed_ranking else (\"None\", 0)\n        best_reliability = reliability_ranking[0] if reliability_ranking else (\"None\", 0)\n        \n        print(f\"ğŸ† Best Speed: {best_speed[0]} ({best_speed[1]:.2f}s)\")\n        print(f\"ğŸ›¡ï¸ Best Reliability: {best_reliability[0]} ({best_reliability[1]:.1f}%)\")\n        \n        if model_stats:\n            avg_success_rate = np.mean([(stats[\"success_count\"] / stats[\"total_count\"]) * 100 \n                                      for stats in model_stats.values()])\n            print(f\"ğŸ“ˆ Overall Success Rate: {avg_success_rate:.1f}%\")\n            \n            avg_response_time = np.mean([np.mean(stats[\"times\"]) \n                                       for stats in model_stats.values() if stats[\"times\"]])\n            print(f\"â±ï¸ Average Response Time: {avg_response_time:.2f}s\")\n\ndef create_evaluation_prompt(results, model_stats):\n    \"\"\"Create comprehensive evaluation prompt for GPT-OSS\"\"\"\n    \n    # Collect all answers for evaluation\n    all_answers = []\n    for question, question_results in results.items():\n        for model, result in question_results.items():\n            if result[\"response_time\"] > 0:  # Only successful responses\n                all_answers.append({\n                    \"question\": question,\n                    \"model\": model,\n                    \"answer\": result[\"answer\"],\n                    \"response_time\": result[\"response_time\"],\n                    \"answer_length\": len(result[\"answer\"])\n                })\n    \n    # Calculate statistics\n    model_performance = {}\n    for model, stats in model_stats.items():\n        if stats[\"times\"]:\n            model_performance[model] = {\n                \"avg_time\": np.mean(stats[\"times\"]),\n                \"min_time\": np.min(stats[\"times\"]),\n                \"max_time\": np.max(stats[\"times\"]),\n                \"success_rate\": (stats[\"success_count\"] / stats[\"total_count\"]) * 100,\n                \"avg_length\": np.mean(stats[\"answer_lengths\"])\n            }\n    \n    # Create evaluation prompt\n    model_stats_text = \"\\n\".join([f\"- {model}: {stats['avg_time']:.2f}s avg, {stats['success_rate']:.1f}% success, {stats['avg_length']:.0f} chars avg\" for model, stats in model_performance.items()])\n    \n    sample_answers_text = \"\"\n    for answer in all_answers[:8]:\n        sample_answers_text += f\"Question: {answer['question']}\\nModel: {answer['model']}\\nTime: {answer['response_time']:.2f}s\\nAnswer: {answer['answer'][:300]}...\\n\\n\"\n    \n    prompt = f\"\"\"Please provide a comprehensive evaluation of these RAG benchmark results:\n\nMODEL PERFORMANCE STATISTICS:\n{model_stats_text}\n\nSAMPLE ANSWERS:\n{sample_answers_text}\n\nPlease evaluate:\n1. Overall system performance and reliability\n2. Speed\n3. Quality of answers\n4. Speed vs quality trade-offs\n\n\nProvide a detailed analysis with specific scores and rankings.\"\"\"\n    \n    return prompt\n\ndef export_results(results):\n    \"\"\"Export results to JSON file\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"gpu_optimized_rag_results_{timestamp}.json\"\n    \n    with open(filename, 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    print(f\"\\nğŸ’¾ Results exported to: {filename}\")\n\nif __name__ == \"__main__\":\n    main() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:12:40.889977Z","iopub.execute_input":"2025-08-06T23:12:40.890209Z","iopub.status.idle":"2025-08-06T23:17:43.849206Z","shell.execute_reply.started":"2025-08-06T23:12:40.890189Z","shell.execute_reply":"2025-08-06T23:17:43.848495Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Using device: cuda\nğŸ® GPU: Tesla T4\nğŸ’¾ GPU Memory: 15.8 GB\nğŸš€ GPU-OPTIMIZED RAG Benchmark for Tesla T4\n============================================================\nâš¡ GPU optimizations applied\n\nğŸ“¦ Checking model availability...\nâœ… gpt-oss:20b ready\nâœ… llama3.1:8b ready\nâœ… gemma3:4b ready\nâœ… qwen3:4b ready\n\nğŸ“ Found 10 files to process\n\nğŸ”„ Step 1: Processing ALL documents with GPU acceleration...\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  10%|â–ˆ         | 1/10 [00:00<00:01,  5.93it/s]","output_type":"stream"},{"name":"stdout","text":"ğŸ“„ Fast text extraction from: /kaggle/input/documents/Is co-branding an effective way to improve brand masstige (1).pdf\nâœ… Direct text extraction successful: 103299 chars\nâœ… Is co-branding an effective way to improve brand masstige (1).pdf: 103069 chars in 0.17s\nğŸ“„ Fast text extraction from: /kaggle/input/documents/Signaling Unobservable Product Quality Through a Brand Ally (1).pdf\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:00<00:00,  7.08it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… Direct text extraction successful: 64817 chars\nâœ… Signaling Unobservable Product Quality Through a Brand Ally (1).pdf: 64816 chars in 0.08s\nğŸ“„ Fast text extraction from: /kaggle/input/documents/Int J Consumer Studies - 2021 - Paydas Turan - Success drivers of cobranding  A metaanalysis (2).pdf\nâœ… Direct text extraction successful: 142521 chars\nâœ… Int J Consumer Studies - 2021 - Paydas Turan - Success drivers of cobranding  A metaanalysis (2).pdf: 142520 chars in 0.18s\nğŸ“„ Fast text extraction from: /kaggle/input/documents/2023 Co-branding as a masstige strategy for luxury brands Desirable or not (1).pdf\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:00<00:00,  9.05it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… Direct text extraction successful: 95264 chars\nâœ… 2023 Co-branding as a masstige strategy for luxury brands Desirable or not (1).pdf: 95042 chars in 0.11s\nğŸ“„ Fast text extraction from: /kaggle/input/documents/An analysis of B2B ingredient co-branding relationships (2).pdf\nâœ… Direct text extraction successful: 71250 chars\nâœ… An analysis of B2B ingredient co-branding relationships (2).pdf: 71249 chars in 0.06s\nğŸ“„ Fast text extraction from: /kaggle/input/documents/Co-branding as a masstige strategy for luxury brands Desirable or not (2).pdf\nâœ… Direct text extraction successful: 95264 chars\nâœ… Co-branding as a masstige strategy for luxury brands Desirable or not (2).pdf: 95042 chars in 0.12s\nğŸ“„ Fast text extraction from: /kaggle/input/documents/Co branding strategies in luxury fashion the Off White case (3).pdf\n","output_type":"stream"},{"name":"stderr","text":"Processing documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  9.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… Direct text extraction successful: 59876 chars\nâœ… Co branding strategies in luxury fashion the Off White case (3).pdf: 59875 chars in 0.10s\nğŸ“„ Fast text extraction from: /kaggle/input/documents/Ingredient branding for a luxury brand- The role of brand and product fit.pdf\nâœ… Direct text extraction successful: 47260 chars\nâœ… Ingredient branding for a luxury brand- The role of brand and product fit.pdf: 47259 chars in 0.05s\nğŸ“„ Fast text extraction from: /kaggle/input/documents/Luxury brands join hands- building interactive alliances on social media (1).pdf\nâœ… Direct text extraction successful: 56730 chars\nâœ… Luxury brands join hands- building interactive alliances on social media (1).pdf: 56729 chars in 0.05s\nğŸ“„ Fast text extraction from: /kaggle/input/documents/The impact of brand value on brand competitiveness (1).pdf\nâœ… Direct text extraction successful: 84188 chars\nâœ… The impact of brand value on brand competitiveness (1).pdf: 84187 chars in 0.08s\n\nğŸ“Š Document Processing Summary:\n   ğŸ“ Total documents: 10\n   â±ï¸ Total extraction time: 1.01s\n   ğŸ“ Total characters: 819,788\n\nğŸ—„ï¸ Step 2: Building global vector store...\nğŸ” Building optimized global vector store...\nğŸ“Š Total chunks to process: 1646\n","output_type":"stream"},{"name":"stderr","text":"Adding to vector store:   0%|          | 0/17 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b1a2d773bb4e6b9f8c0e58d4753e24"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:   6%|â–Œ         | 1/17 [00:00<00:06,  2.48it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c1579ce3ffa44789d70b7676a72ab5e"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  12%|â–ˆâ–        | 2/17 [00:00<00:04,  3.02it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de6d88986f8d49099239ba32f4937cc8"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  18%|â–ˆâ–Š        | 3/17 [00:00<00:04,  3.12it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd28839b797f4e78a348800e389a88f8"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  24%|â–ˆâ–ˆâ–       | 4/17 [00:01<00:03,  3.26it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e26a16da5bc5483a9196b4b4dc2ac648"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:01<00:03,  3.32it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7fbe27344404093a5066546d7fd20a8"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:01<00:03,  3.34it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb89e7f0126449e9860469d2a2f9c28e"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:02<00:02,  3.36it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66d1aeee5f1f48aeb0f67c23f037f935"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:02<00:02,  3.23it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aa45fdcf0734fb2bf2fc9ab43478da6"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:02<00:02,  3.26it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c2cb74a38be48bebd87ba1a7877680b"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:03<00:02,  3.22it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d000049c9df944e3b838e7c9013ebd20"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:03<00:01,  3.07it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb4b96437ab94c83939deaa298a1a93e"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:03<00:01,  3.14it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c6c1dc5b75649638ebcf250659334f5"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:04<00:01,  3.15it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e615e3f6ba07414ea17950cf445d65d3"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:04<00:00,  3.24it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84919043727a4e1a9b0a24dafa62548b"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:04<00:00,  3.30it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25df5c3b381842779604c977c5de9f9c"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:04<00:00,  3.34it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c50acd7bf041fc85dcd3c7e1bc1259"}},"metadata":{}},{"name":"stderr","text":"Adding to vector store: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:05<00:00,  3.33it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… Global vector store ready in 5.26s\n\nğŸ¤– Step 3: Processing 4 questions with 4 models...\n\n============================================================\nğŸ“Œ Question 1/4: what is co-branding?\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"495e714e96bc4051be0a8fc6051e0e4e"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Search time: 0.05s\nğŸ“„ Relevant documents: An analysis of B2B ingredient co-branding relationships (2).pdf\n\nğŸ¤– Model 1/4: gpt-oss:20b\n----------------------------------------\nğŸ§  Asking gpt-oss:20b (optimized)...\nâ±ï¸ Response time: 10.09s\nğŸ“ Answer: Coâ€‘branding is a marketing strategy in which two or more independent brands are presented together on a single product or service. By combining their identities, the brands aim to create a new offering that leverages the strengths and reputations of each partner. This approach is also known as joint branding, brand alliances, or symbiotic marketing. Companies use coâ€‘branding to achieve operational benefits, such as shared resources or expertise, and to capitalize on the â€œspillâ€‘overâ€ effect where the positive perception of one brand boosts the other. In a businessâ€‘toâ€‘business context, coâ€‘branding can help firms expand their reach, enter new markets, or enhance credibility by association. Ultimately, the goal is to deliver added value to consumers while strengthening the market position of all involved brands.\nğŸ“ Answer length: 819 characters\n\nğŸ¤– Model 2/4: llama3.1:8b\n----------------------------------------\nğŸ§  Asking llama3.1:8b (optimized)...\nâ±ï¸ Response time: 9.25s\nğŸ“ Answer: Co-branding is a marketing strategy that involves presenting two or more independent brands jointly on the same product or service. It has also been referred to as co-marketing, joint branding, brand alliances, and symbiotic marketing. Co-branding allows companies to combine their resources and expertise to create a new offering that can provide operational benefits and leverage the \"spill over\" effect of one brand onto another. This strategy is often used in business-to-business relationships where two or more companies collaborate on a product or service. By co-branding, companies can increase their visibility, credibility, and market share. Co-branding requires careful planning and management to ensure that both brands benefit from the partnership. Overall, co-branding is a strategic approach to marketing that involves combining multiple brands to achieve common goals.\nğŸ“ Answer length: 884 characters\n\nğŸ¤– Model 3/4: gemma3:4b\n----------------------------------------\nğŸ§  Asking gemma3:4b (optimized)...\nâ±ï¸ Response time: 12.82s\nğŸ“ Answer: Co-branding is a strategic marketing approach that involves combining the efforts of two or more separate brands onto a single product or service offering. Essentially, itâ€™s a deliberate tactic where independent brands work together to present a unified identity. This strategy is often referred to by various names like co-marketing, joint branding, or brand alliances, highlighting its multifaceted nature. Companies utilize co-branding to achieve operational efficiencies and leverage the positive associations of each brand involved. The core idea is to capitalize on the \"spill over\" effect, where one brand's reputation boosts the otherâ€™s visibility and appeal to consumers.  Ultimately, co-branding aims to create a stronger market presence and potentially expand reach by uniting the strengths of multiple brands within a single offering.\nğŸ“ Answer length: 846 characters\n\nğŸ¤– Model 4/4: qwen3:4b\n----------------------------------------\nğŸ§  Asking qwen3:4b (optimized)...\nâ±ï¸ Response time: 24.09s\nğŸ“ Answer: Let me look at the context carefully. The context mentions that co-branding is \"the strategy of presenting two or more independent brands jointly on the same product or service. \" It also lists alternative terms like co-marketing, joint branding, brand alliances, and symbiotic marketing. The context starts with \"nt co-branding\" which seems like a typo - probably meant \"Co-branding\" but I'll focus on the clear definition given. It also mentions co-branding is adopted for reasons like operational benefits and \"spill over\" effects, though the text gets cut off at \"spill over\" effect. I should start with the core definition from the context, then mention the alternative terms, explain why it's used (operational benefits and spill-over effects), and note that it's a business-to-business strategy as per the context header. The user specified \"Business-to-business\" in the context title, so I should include that it's particularly relevant for B2B contexts. I'll make sure to avoid adding information not in the context. The context doesn't give examples or deeper details, so I'll stick to what's provided. I'll count my sentences carefully to hit 6-7. Let me outline: 1) Definition, 2) Alternative terms, 3) Purpose (operational benefits), 4) Spill-over effects, 5) B2B relevance, 6) Why it's used, 7) Conclusion about strategic value. That should work. The user seems to be a student or professional needing a concise academic answer - probably for an assignment given the structured context. They want accuracy without fluff. I'll keep it professional but simple. No need for citations since the context references Abratt & Motlana but the answer doesn't require it per the query. Okay, drafting now: First sentence defines co-branding, second lists terms, third explains operational benefits, fourth covers spill-over, fifth specifies B2B context, sixth gives the strategic reason, seventh wraps up with its purpose. That's 7 sentences - perfect. I'll double-check for clarity and that I'm not adding external knowledge. Co-branding is a strategic approach where two or more independent brands collaborate to present themselves jointly on the same product or service. This practice is also known by various terms such as co-marketing, joint branding, brand alliances, and symbiotic marketing. It is primarily adopted by businesses to achieve operational benefits and leverage the \"spill over\" effects that enhance market reach and credibility. The context specifically highlights that co-branding is particularly relevant within business-to-business relationships, where such partnerships can create mutual value. Organizations utilize this strategy to strengthen their market position by combining the strengths, resources, and customer bases of different brands. Co-branding serves as a strategic mechanism to address specific business challenges while simultaneously expanding the potential audience and enhancing the perceived quality of the offering. Ultimately, it functions as a valuable tool for businesses seeking to build stronger, more effective partnerships in competitive markets.\nğŸ“ Answer length: 3103 characters\n\n============================================================\nğŸ“Œ Question 2/4: what is luxury co-branding?\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b39955bbbb348208f95b140b3f3b2dc"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Search time: 0.02s\nğŸ“„ Relevant documents: 2023 Co-branding as a masstige strategy for luxury brands Desirable or not (1).pdf\n\nğŸ¤– Model 1/4: gpt-oss:20b\n----------------------------------------\nğŸ§  Asking gpt-oss:20b (optimized)...\nâ±ï¸ Response time: 11.83s\nğŸ“ Answer: Luxury coâ€‘branding refers to a strategic partnership in which a highâ€‘end, prestige brand partners with another brandâ€”often a more accessible or massâ€‘market brandâ€”to create a joint product or marketing campaign. The goal is to combine the luxury brandâ€™s perceived quality, heritage, and exclusivity with the partnerâ€™s broader reach, distribution network, or complementary product attributes. By doing so, the luxury brand can tap into new customer segments and generate additional revenue streams while maintaining its premium positioning. The partner brand, in turn, gains a halo effect, benefiting from the association with the luxury image and attracting consumers who aspire to the status symbol. This collaboration is typically framed as a â€œmasstigeâ€ strategy, where the product is marketed as a massâ€‘market luxury that offers aspirational value at a more attainable price point. Successful luxury coâ€‘branding requires careful alignment of brand values, consistent quality standards, and clear communication of the joint productâ€™s unique selling proposition to avoid diluting the luxury brandâ€™s core equity.\nğŸ“ Answer length: 1111 characters\n\nğŸ¤– Model 2/4: llama3.1:8b\n----------------------------------------\nğŸ§  Asking llama3.1:8b (optimized)...\nâ±ï¸ Response time: 10.10s\nğŸ“ Answer: Luxury co-branding refers to the practice of partnering between a luxury brand and another brand or entity to create a new product or service that combines the strengths of both partners. This strategy aims to leverage the prestige and desirability associated with luxury brands, while also expanding their reach and appeal to a wider audience. Co-branding can take various forms, such as collaborations on limited-edition products, joint marketing campaigns, or even co-ownership of a new business venture. The goal is often to create a \"masstige\" product that appeals to both the luxury brand's loyal customers and a broader market segment. By partnering with other brands, luxury companies can tap into new markets, increase their visibility, and enhance their reputation. However, this strategy also carries risks, such as diluting the luxury brand's exclusivity or compromising its values. Effective co-branding requires careful selection of partners and management of the partnership to maintain the integrity and prestige of both brands involved.\nğŸ“ Answer length: 1053 characters\n\nğŸ¤– Model 3/4: gemma3:4b\n----------------------------------------\nğŸ§  Asking gemma3:4b (optimized)...\nâ±ï¸ Response time: 13.33s\nğŸ“ Answer: Based on the provided context, luxury co-branding emerges as a strategic approach primarily utilized by luxury brands seeking to enhance their perceived value and desirability. This strategy, termed a â€œmasstigeâ€ (mass-to-status) approach, involves partnering with brands that possess a certain level of prestige or recognition, even if they arenâ€™t inherently luxury themselves. The core idea is to leverage the established brand equity of the collaborator to subtly elevate the image of the luxury brand in question. Essentially, the co-branding relationship aims to bridge the gap between accessible and aspirational luxury, offering consumers a more attainable entry point into a premium brand experience.  The research, published in the Journal of Business Research, specifically investigates whether this tactic is ultimately beneficial or detrimental to luxury brandsâ€™ core identity and positioning.  Therefore, luxury co-branding represents a calculated attempt to broaden appeal and manage brand perception within the competitive luxury market.\nğŸ“ Answer length: 1051 characters\n\nğŸ¤– Model 4/4: qwen3:4b\n----------------------------------------\nğŸ§  Asking qwen3:4b (optimized)...\nâ±ï¸ Response time: 24.69s\nğŸ“ Answer: They've provided the title and some metadata from the Journal of Business Research (2023) but no actual content of the article. The question is about defining luxury co-branding, but without reading the actual paper, I can't give a precise answer from this source. The user seems to be a student or researcher who might be working on a business assignment. They probably need this for academic purposes. I should be transparent about the limitation while still providing useful information. Looking at the title \"Co-branding as a masstige strategy for luxury brands: Desirable or not?\", I can infer that the paper discusses co-branding specifically for luxury brands as a \"masstige\" strategy. Masstige refers to luxury brands using collaborations to create perceived status without being exclusive. But I must emphasize that without the article's content, I can't give a definitive answer. The best I can do is explain what co-branding generally means in luxury contexts based on the title's implication, while being clear this isn't from the paper itself. I'll structure my response to: 1) state the limitation clearly, 2) explain what the title suggests about luxury co-branding, 3) note that the actual definition would be in the paper, and 4) keep it to 6-7 sentences as requested. The user might be frustrated if they expected a direct answer, so I'll make sure to be helpful while staying honest about the information gap. Academic integrity matters here - I shouldn't pretend to know what's in the paper when it's not provided. Based solely on the provided context from the *Journal of Business Research* (2023) title \"Co-branding as a masstige strategy for luxury brands: Desirable or not?\", I cannot definitively state what luxury co-branding is, as the actual article content is not included in the given metadata. The title itself suggests that the research investigates co-branding specifically within the luxury sector as a potential \"masstige\" strategyâ€”meaning an approach where luxury brands collaborate with other entities to create a broader, more accessible perception of status without sacrificing exclusivity. However, without reading the paper's full text, I cannot provide the precise definition or operationalization used by the authors. The context only indicates the paper's focus area (luxury brands) and a key theoretical concept (masstige), not the explicit definition of luxury co-branding itself. Therefore, the answer must be framed as an inference from the title, acknowledging that the true definition would be found in the article's methodology and findings section. For an accurate definition, the full paper would be necessary. (6 sentences).\nğŸ“ Answer length: 2678 characters\n\n============================================================\nğŸ“Œ Question 3/4: What are the factors affecting co-branding?\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32473b3e8f054ae493e1e25b3233cc43"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Search time: 0.02s\nğŸ“„ Relevant documents: Is co-branding an effective way to improve brand masstige (1).pdf\n\nğŸ¤– Model 1/4: gpt-oss:20b\n----------------------------------------\nğŸ§  Asking gpt-oss:20b (optimized)...\nâ±ï¸ Response time: 34.13s\nğŸ“ Answer: Coâ€‘branding success hinges on several interrelated factors. First, **brand fit or compatibility** is crucial; the two partners must share similar values, target audiences, and product categories so that the joint offering feels coherent to consumers. Second, the **existing brand equity and reputation** of each partner influence how much credibility and trust the collaboration can generateâ€”stronger equity can amplify perceived value while weak equity may dilute it. Third, **consumer perception and involvement** matter; customers must see the partnership as authentic, highâ€‘quality, and novel, rather than a gimmick. Fourth, **brand personality and image alignment** help avoid cognitive dissonanceâ€”complementary personalities reinforce the desired image and make the alliance feel natural. Fifth, **market positioning and differentiation** must be clear; coâ€‘branding should enhance the productâ€™s distinctiveness from competitors while preserving each brandâ€™s unique positioning. Sixth, **strategic fit and synergy**â€”such as shared distribution channels, complementary capabilities, or cost savingsâ€”must be tangible to justify the partnership. Finally, effective **brand management and governance** (clear roles, consistent communication, and control over brand usage) are essential to maintain consistency and protect both brandsâ€™ identities.\nğŸ“ Answer length: 1347 characters\n\nğŸ¤– Model 2/4: llama3.1:8b\n----------------------------------------\nğŸ§  Asking llama3.1:8b (optimized)...\nâ±ï¸ Response time: 9.89s\nğŸ“ Answer: The article \"1 + 1 > 2? Is co-branding an effective way to improve brand masstige?\" explores various factors that affect the effectiveness of co-branding. Co-branding is a marketing strategy where two or more brands collaborate to create a new product or service, with the goal of increasing brand value and reputation. The study identifies several key factors that influence the success of co-branding, including brand compatibility, consumer perception, and market conditions. Additionally, the authors examine the role of brand masstige, which refers to the prestige and status associated with a particular brand. They also consider the impact of co-branding on brand equity and customer loyalty. Furthermore, the study highlights the importance of selecting compatible brands that complement each other's strengths and weaknesses. Overall, the article provides insights into the complexities of co-branding and its potential benefits for businesses seeking to enhance their brand reputation and value.\nğŸ“ Answer length: 1005 characters\n\nğŸ¤– Model 3/4: gemma3:4b\n----------------------------------------\nğŸ§  Asking gemma3:4b (optimized)...\nâ±ï¸ Response time: 13.20s\nğŸ“ Answer: Based on the provided excerpt from the Journal of Business Research, the research investigates the effectiveness of co-branding in relation to â€œbrand masstige,â€ which refers to the combination of high quality and low price. The study, authored by Shan, Lu, and Cui, explores whether co-branding strategies can positively influence this blended brand image. While the specific factors impacting co-branding aren't explicitly detailed in this brief introduction, the core question posed â€“ â€œIs co-branding an effective way to improve brand masstige?â€ â€“ suggests that several elements likely play a crucial role. These could include the selection of complementary brands, the alignment of brand values, the perceived quality of each brand involved, and ultimately, the strategic execution of the co-branding campaign itself.  Further research within the article would undoubtedly delve deeper into these and potentially other variables impacting the success of co-branding initiatives.\nğŸ“ Answer length: 981 characters\n\nğŸ¤– Model 4/4: qwen3:4b\n----------------------------------------\nğŸ§  Asking qwen3:4b (optimized)...\nâ±ï¸ Response time: 26.03s\nğŸ“ Answer: They want me to answer a question about co-branding factors based on a journal article context they provided. The context is from the Journal of Business Research (2022) with a title about whether co-branding improves brand masstige. It's just the journal's metadata and author information - no abstract, no content, no discussion of factors. The title mentions \"1 + 1 > 2?\" which suggests the article is testing if co-branding actually increases brand masstige, but there's no actual content here. The user might be confused or testing me. They could be a student or researcher who thinks this context contains the answer, but it doesn't. I should clarify that the provided context is insufficient for answering this question. I recall that in academic contexts, journal metadata like this usually doesn't include the full article content. The \"ARTICLE INFO\" section they showed is just standard formatting. Without the abstract or full text, I can't extract any factors affecting co-branding. For my response, I'll need to be clear and helpful. I'll state that the context doesn't contain the answer, explain why, and suggest what they might need to find the actual answer. I'll keep it to 6-7 sentences as requested. The user might be frustrated if they expected an answer, so I'll phrase it politely - emphasizing that this is a common issue with journal metadata. I'll mention that typical factors from such studies (like brand alignment, consumer perception) could be relevant but I can't specify them without the actual content. I'll structure it: 1) State context lacks answer, 2) Explain why, 3) Note what the title implies, 4) Suggest where to find real factors, 5) Give examples of common factors (to be helpful), 6) Reiterate need for full text, 7) Offer to help further if they have more context. This keeps it professional and within the sentence limit while addressing what they probably need. The provided context from the Journal of Business Research (2022) only includes standard journal metadata, author information, and the article titleâ€”it does not contain the abstract, full text, or any discussion of factors affecting co-branding. Therefore, this context cannot answer the question about what factors influence co-branding. The title \"1 + 1 > 2? Is co-branding an effective way to improve brand masstige?\" suggests the article investigates whether co-branding enhances brand masstige, but without the article's content, specific factors like brand alignment, consumer perception, strategic fit, or resource constraints cannot be identified. To determine actual factors affecting co-branding, you would need access to the article's abstract or full text, which is not present here. Common factors in co-branding literature typically include brand compatibility, shared customer bases, perceived value, and strategic objectives, but these are general knowledge and not derived from this specific context. This journal article likely addresses whether co-branding improves brand masstige (a measure of brand prestige), but the factors influencing *co-branding itself*â€”such as implementation challenges or success conditionsâ€”remain unstated in the provided metadata. For an accurate answer, consult the article's full text or related research on co-branding strategies.\nğŸ“ Answer length: 3289 characters\n\n============================================================\nğŸ“Œ Question 4/4: What are the types of co-branding?\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd98f9a7441141c09f73a14cd7b1e2bc"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Search time: 0.02s\nğŸ“„ Relevant documents: Int J Consumer Studies - 2021 - Paydas Turan - Success drivers of cobranding  A metaanalysis (2).pdf\n\nğŸ¤– Model 1/4: gpt-oss:20b\n----------------------------------------\nğŸ§  Asking gpt-oss:20b (optimized)...\nâ±ï¸ Response time: 20.66s\nğŸ“ Answer: Coâ€‘branding can be grouped into three broad categories that are widely recognized in the literature.  \n1. **Product coâ€‘branding** occurs when two or more brands are combined on a single tangible product (for example, a sports shoe that carries both a footwear and a sportsâ€‘wear brand).  \n2. **Service coâ€‘branding** involves the joint promotion of a service, such as a hotel partnering with a creditâ€‘card company to offer exclusive benefits to guests.  \n3. **Corporate (or umbrella) coâ€‘branding** refers to the partnership of two corporate entities that share a common brand identity, often seen in joint ventures or strategic alliances where both companiesâ€™ logos appear together on marketing materials.  \n\nThese three types capture the main ways brands collaborate to create new value propositions, each with its own set of success factors and strategic considerations.\nğŸ“ Answer length: 870 characters\n\nğŸ¤– Model 2/4: llama3.1:8b\n----------------------------------------\nğŸ§  Asking llama3.1:8b (optimized)...\nâ±ï¸ Response time: 9.96s\nğŸ“ Answer: Unfortunately, the provided context does not mention the specific types of co-branding. However, based on general knowledge about co-branding, it can be inferred that there are several types of co-branding, including:\n\nCo-branding is a marketing strategy where two or more brands collaborate to create a new product, service, or experience. The three categories mentioned in the context refer to the sub-group variables assigned to success factors of co-branding, not the types of co-branding itself.\n\nHowever, some common types of co-branding include:\n\n* Product co-branding: where two brands collaborate on a single product\n* Service co-branding: where two brands collaborate on a service offering\n* Cause co-branding: where two brands partner to support a social or environmental cause\n\nIt's worth noting that the context provided does not explicitly mention these types of co-branding, but rather focuses on analyzing success factors and categorizing them.\nğŸ“ Answer length: 960 characters\n\nğŸ¤– Model 3/4: gemma3:4b\n----------------------------------------\nğŸ§  Asking gemma3:4b (optimized)...\nâ±ï¸ Response time: 12.47s\nğŸ“ Answer: Based on the provided text, the research focuses on analyzing the success factors of co-branding, aiming to consolidate a fragmented body of literature on the topic. This meta-analysis categorizes the various success factors related to co-branding into three distinct sub-group variables. While the text doesn't explicitly detail the specific types of co-branding, it indicates a systematic approach to understanding what drives positive evaluations. The research appears to be organizing the identified success factors into a structured framework for analysis. Therefore, the study is essentially grouping success factors into three categories, suggesting a tiered approach to understanding the nuances of co-branding effectiveness. Further investigation would be needed to determine the precise nature of these three categories.\nğŸ“ Answer length: 830 characters\n\nğŸ¤– Model 4/4: qwen3:4b\n----------------------------------------\nğŸ§  Asking qwen3:4b (optimized)...\nâ±ï¸ Response time: 23.68s\nğŸ“ Answer: They need a 6-7 sentence response. It mentions \"articles are governed by the applicable Creative Commons License\" and then has some text about research on co-branding success factors. The key part seems to be: \"First, this research provides a robust analysis of all the studied success factors that lead to the positive evaluation of co-Â­brand as the positive evaluation of co-Â­branding\" - but it cuts off mid-sentence with \"brand chara\". I notice the context doesn't actually specify what the types of co-branding are. The text is talking about success factors and categories for co-branding evaluation, but it doesn't list the types. The phrase \"assign the subgroup variables to one of three categories\" suggests there are three categories, but it doesn't name them. This is tricky because the user is asking about \"types of co-branding\" but the context doesn't provide that information. The context is focused on success factors and evaluation criteria, not classification of co-branding types. I should be honest with the user that the context doesn't contain information about co-branding types. I'll explain that the text discusses success factors and categories for evaluation, but doesn't specify what those types are. I'll make sure to keep it to 6-7 sentences as requested, and be clear that I can't answer the question based on this limited context. The user might be a student or researcher who needs this for academic work. They probably expected the context to contain the answer, but it seems like the context they provided is incomplete or mislabeled. I'll be helpful by explaining what the context does say about co-branding research while being transparent about the missing information. I'll structure my response to first state that the context doesn't specify types, then explain what the context does mention about co-branding research, and finally suggest that the actual types might be elsewhere in the full document. This keeps it factual and within the sentence limit they requested. The provided context does not specify the types of co-branding. Instead, it describes a research meta-analysis that identifies success factors for positive co-branding evaluation and categorizes these factors into three subgroups. The text explicitly states that the study \"assigns the subgroup variables to one of three categories\" but does not name or define what those categories represent in terms of co-branding types. This context focuses on analytical frameworks for evaluating co-branding effectiveness rather than classifying the different types of co-branding relationships themselves. Therefore, based solely on this excerpt, it is impossible to determine the specific types of co-branding as the answer requires information that is absent here. The research discussed deals with success factors and evaluation criteria, not the typology of co-branding structures. To answer the question about co-branding types, additional context from the full document would be necessary.\nğŸ“ Answer length: 2995 characters\n\n================================================================================\nğŸ“Š COMPREHENSIVE EVALUATION & COMPARISON\n================================================================================\n\nğŸ¯ OVERALL STATISTICS\n----------------------------------------\nğŸ“ Total questions processed: 4\nğŸ¤– Total models tested: 4\n\nğŸ¤– MODEL PERFORMANCE COMPARISON\n----------------------------------------\nğŸ“Š gpt-oss:20b:\n   â±ï¸ Response time: 19.18s avg (10.09s - 34.13s)\n   âœ… Success rate: 100.0% (4/4)\n   ğŸ“ Avg answer length: 1037 characters\nğŸ“Š llama3.1:8b:\n   â±ï¸ Response time: 9.80s avg (9.25s - 10.10s)\n   âœ… Success rate: 100.0% (4/4)\n   ğŸ“ Avg answer length: 976 characters\nğŸ“Š gemma3:4b:\n   â±ï¸ Response time: 12.95s avg (12.47s - 13.33s)\n   âœ… Success rate: 100.0% (4/4)\n   ğŸ“ Avg answer length: 927 characters\nğŸ“Š qwen3:4b:\n   â±ï¸ Response time: 24.62s avg (23.68s - 26.03s)\n   âœ… Success rate: 100.0% (4/4)\n   ğŸ“ Avg answer length: 3016 characters\n\nğŸ† PERFORMANCE RANKINGS\n----------------------------------------\nğŸš€ Speed Ranking (Fastest to Slowest):\n   1. llama3.1:8b: 9.80s\n   2. gemma3:4b: 12.95s\n   3. gpt-oss:20b: 19.18s\n   4. qwen3:4b: 24.62s\n\nğŸ›¡ï¸ Reliability Ranking (Most to Least Reliable):\n   1. gpt-oss:20b: 100.0% success rate\n   2. llama3.1:8b: 100.0% success rate\n   3. gemma3:4b: 100.0% success rate\n   4. qwen3:4b: 100.0% success rate\n\nğŸ¤– GPT-OSS FINAL EVALUATION\n----------------------------------------\nğŸ§  Asking gpt-oss:20b for final evaluation...\nğŸ§  Asking gpt-oss:20b (optimized)...\nğŸ“Š GPT-OSS Evaluation (29.16s):\nThe benchmark shows that all four models achieve 100â€¯% success, giving them a perfect reliability score of 10/10. In terms of speed, llamaâ€¯3.1â€¯(8â€¯B) is the clear leader with an average latency of 9.8â€¯s, followed by gemmaâ€¯3â€¯(4â€¯B) at 12.95â€¯s, gptâ€‘ossâ€¯(20â€¯B) at 19.18â€¯s, and qwenâ€¯3â€¯(4â€¯B) lagging at 24.62â€¯s; a simple speed ranking would therefore be llamaâ€¯3.1 > gemmaâ€¯3 > gptâ€‘oss > qwenâ€¯3. Assigning a 1â€‘10 speed score yields 9, 8, 7, and 4 respectively. Quality of answers, judged by coherence, completeness, and relevance, is uniformly high across the board, so each model can be given a 9/10 quality score, with qwenâ€¯3 slightly lower at 7/10 due to its verbose and occasionally tangential responses. The speedâ€‘vsâ€‘quality tradeâ€‘off is negligible for llamaâ€¯3.1, gemmaâ€¯3, and gptâ€‘oss, where fast responses do not compromise answer quality; however, qwenâ€¯3â€™s slower latency is offset by a modest drop in answer quality, making its tradeâ€‘off less favorable. Overall, the ranking that balances speed, reliability, and quality is: 1) llamaâ€¯3.1, 2) gemmaâ€¯3, 3) gptâ€‘oss, and 4) qwenâ€¯3.\n\nğŸ’¾ Results exported to: gpu_optimized_rag_results_20250806_231743.json\n","output_type":"stream"}],"execution_count":8}]}